---
title: Identifying Spam/Ham Messages
date: 2018-10-15
tags: 
  - Machine Learning
  - Text Analytics
  - Classification
header:
  image: "/images/SMS Spam/email.jpg"
  teaser: "/images/SMS Spam/email.jpg"
excerpt: "Machine Learning, Text Analytics, Classification"
mathjax: "true"
---
The aim is to make predictions on the sms messages and categorize them into spam or ham. 
This dataset was taken from UCI SMS Spam Collection dataset.
## 1) Setup
### Load packages
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```
### Load & Viewing the Data
```python
sms = pd.read_csv('SMSSpamCollection', sep='\t', names=["label", "message"])
sms.head()
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
    </tr>
  </tbody>
</table>

## 2) Exploratory data analysis
### View some statistics of the data
```python
sms.describe()
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5572</td>
      <td>5572</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>2</td>
      <td>5169</td>
    </tr>
    <tr>
      <th>top</th>
      <td>ham</td>
      <td>Sorry, I'll call later</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>4825</td>
      <td>30</td>
    </tr>
  </tbody>
</table>

### View the statistics against the 2 unique label
```python
sms.groupby('label').describe()
```  
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="4" halign="left">message</th>
    </tr>
    <tr style="text-align: right;">
      <th>label</th>
      <th>count</th>
      <th>unique</th>
      <th>top</th>
      <th>freq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ham</th>
      <td>4825</td>
      <td>4516</td>
      <td>Sorry, I'll call later</td>
      <td>30</td>
    </tr>
    <tr>
      <th>spam</th>
      <td>747</td>
      <td>653</td>
      <td>Please call our customer service representativ...</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
### View the count of unique labels on a piechart
```python
sms["label"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)
plt.ylabel("Spam vs Ham")
plt.legend(["Ham", "Spam"])
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SMS Spam/piechart.jpg" alt="">

### Insert a new column to detect how long the text messages are
```python
sms['length'] = sms['message'].apply(len)
sms.head()
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
      <td>111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
      <td>29</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
      <td>155</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
      <td>49</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
      <td>61</td>
    </tr>
  </tbody>
</table>
### View the frequency count on a histogram
```python
sms['length'].plot(bins=50, kind='hist') 
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SMS Spam/hist.jpg" alt="">
<br/>
### Determine if message length is a distinguishing feature between ham and spam
```python
sms.hist(column='length', by='label', bins=50,figsize=(12,4))
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SMS Spam/hist2.jpg" alt="">
<br/>
Seems like we discover a trend that spam messages tend to have more characters.

### Word Cloud
Create a word cloud to see which are the words frequently occurring in spam or ham messages
```python
from wordcloud import WordCloud, STOPWORDS
from os import path
from PIL import Image

# Separate sms into spam/ham
spam = sms[sms['label'] == 'spam']
ham = sms[sms['label'] == 'ham']
```
Spam
```python
stopwords_1 = set(STOPWORDS)
k = (' '.join(spam['message']))

# Input mask image
spam_mask = np.array(Image.open(path.join("Spam.png")))
# Generate a word cloud image
wordcloud = WordCloud(background_color="white", colormap='plasma', mask=spam_mask, width = 2500, height = 500, collocations=False, 
                      max_words=300, stopwords=stopwords_1, relative_scaling=0.2).generate(k)
plt.figure(figsize=(15,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.grid(False)
plt.tight_layout()
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SMS Spam/spamcloud.jpg" alt="">
Ham
```python
stopwords_1 = set(STOPWORDS)
k = (' '.join(ham['message']))

# Input mask image
ham_mask = np.array(Image.open(path.join("Ham.png")))

# Generate a word cloud image
wordcloud = WordCloud(background_color="white", colormap='plasma', mask=ham_mask, width = 2500, height = 500, collocations=False, 
                      max_words=300, stopwords=stopwords_1, relative_scaling=0.2).generate(k)
plt.figure(figsize=(15,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.grid(False)
plt.tight_layout()
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SMS Spam/hamcloud.jpg" alt="">
* Words such as 'FREE', 'reply', 'claim', 'prize' occurred often in spam messages
* Words such as 'go', 'will', 'Ok', 'good' occurred often in ham messages
* The word 'call' and 'now' seems to be occurring frequently in both spam and ham messages.

## 3) Text Preprocessing
### Define a function for processing the text messages
```python
import string
from nltk.corpus import stopwords

def text_process(text):
    # Remove any punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove any stopwords
    # NLTK's stopwords assumes words are all lowercased
    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]

    #Join the characters again to form the string
    return " ".join(text)
```
### Apply the function on the text messages
```python
messages = sms['message'].apply(text_process)
messages.head()
```
{% highlight text %}
0    Go jurong point crazy Available bugis n great ...
1                              Ok lar Joking wif u oni
2    Free entry 2 wkly comp win FA Cup final tkts 2...
3                  U dun say early hor U c already say
4          Nah dont think goes usf lives around though
Name: message, dtype: object
{% endhighlight %}

### Bag of words
Create a set of features indicating the number of times an observation's text contains a particular word, using a bag of words model.
Bag of words models output a feature for every unique word in text data, with each feature containing a count of occurrences in observations. The output would results in a matrix that can contain thousands of features and it would likely be a sparse matrix as most words do not occur in most observations.
```python
from sklearn.feature_extraction.text import CountVectorizer
# Create a bag of words feature matrix
count = CountVectorizer()
bag_of_words = count.fit_transform(messages)
```
### View the shape of the feature matrix
```python
bag_of_words.shape
```
{% highlight text %}
(5572, 9437)
{% endhighlight %}
We had 9437 features.
### View the matrix
```python
bag_of_words.toarray()
```
{% highlight text %}
array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)
{% endhighlight %}
Indeed it's a sparse matrix.

### TF-IDF
Weight the words by their importance to an observation by the method of term frequencyâ€“inverse document frequency.
<br/>
$$tf{\text -}idf(t,d) = tf(t,d)\times idf(t)$$
```python
from sklearn.feature_extraction.text import TfidfTransformer
tfidf = TfidfTransformer()
tfidf_bag_of_words = tfidf.fit_transform(bag_of_words)
```

## 4) Training
Spliting the data into 8:2 for training and testing
```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(tfidf_bag_of_words, sms['label'], test_size = 0.20, random_state=5)
```
Import libraries
```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
```
Selecting a suite of different algorithms capable of working on classification problem.
```python
models = []
models.append(('LR', LogisticRegression()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB',  MultinomialNB()))
models.append(('SVM', SVC()))
models.append(('AB', AdaBoostClassifier()))
models.append(('GBM', GradientBoostingClassifier()))
models.append(('RF', RandomForestClassifier()))
models.append(('ET', ExtraTreesClassifier()))
```
Setting a 10-fold cross validation and evaluate using the accuracy scoring metric.
```python
num_folds = 10
seed = 5
scoring = 'accuracy'
```
Displaying the mean and standard deviation of accuracy for each algorithm
```python
results = []
names = []
for name, model in models:
    kfold = KFold(n_splits=num_folds, random_state=seed)
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)
```
{% highlight text %}
LR: 0.943908 (0.007231)
KNN: 0.900163 (0.007247)
CART: 0.954683 (0.010586)
NB: 0.958044 (0.004814)
SVM: 0.864933 (0.010775)
AB: 0.964775 (0.008023)
GBM: 0.961186 (0.007569)
RF: 0.963880 (0.008164)
ET: 0.966346 (0.004364)
{% endhighlight %}
Viewing the distribution of accuracy values on a boxplot
```python
fontsize = 22
fig = plt.figure(figsize=(26,8))
fig.suptitle('Algorithm Comparison', fontsize=fontsize)
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.grid(linewidth=1, alpha=0.3, color='lightgrey')
plt.xticks(fontsize=fontsize)
plt.yticks(fontsize=fontsize)
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/SMS Spam/boxplot.jpg" alt="">
The ExtraTreesClassifier seems to be performing well here, but we would like to test out the multinomial naive bayes classifier, as naive bayes have been demonstrated to be fast, reliable and accurate in a number of applications of NLP, given its simple nature.
It is a popular supervised learning method for text classification that performs well despite its â€˜naiveâ€™ assumptions of independence between features.

### Tuning
We would like to improve the performance of naive bayes by tuning its parameter.
There are tuning parameter in NB for using multinominal distribution here, called the Lapalace smoothing parameter, alpha.
```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

alpha = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
param_grid = dict(alpha=alpha)
model = MultinomialNB()
kfold = KFold(n_splits=num_folds, random_state=seed)
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result = grid.fit(X_train, y_train)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
```
{% highlight text %}
Best: 0.978461 using {'alpha': 0.3}
0.975993 (0.008286) with: {'alpha': 0.05}
0.975544 (0.008242) with: {'alpha': 0.1}
0.977788 (0.006060) with: {'alpha': 0.2}
0.978461 (0.005602) with: {'alpha': 0.3}
0.977115 (0.007881) with: {'alpha': 0.4}
0.975993 (0.006014) with: {'alpha': 0.5}
0.972627 (0.006080) with: {'alpha': 0.6}
0.967243 (0.005505) with: {'alpha': 0.7}
0.963653 (0.005187) with: {'alpha': 0.8}
0.960063 (0.004471) with: {'alpha': 0.9}
0.958044 (0.004816) with: {'alpha': 1}
{% endhighlight %}
We can see that the optimal configuration is an alpha value of 0.3.

## 5) Model testing
Fitting the NB model with the optimal parameter on the training data and make predictions for the test dataset
```python
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

model = MultinomialNB(alpha = 0.3)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
print("1) Accuracy score: {0:.4f}\n".format(accuracy_score(y_test, predictions)))
print("2) Confusion Matrix: \n {}\n".format(confusion_matrix(y_test, predictions)))
print("3) Classification Report: \n {}".format(classification_report(y_test, predictions)))
```
{% highlight text %}
1) Accuracy score: 0.9857

2) Confusion Matrix: 
 [[965   5]
 [ 11 134]]

3) Classification Report: 
              precision    recall  f1-score   support

        ham       0.99      0.99      0.99       970
       spam       0.96      0.92      0.94       145

avg / total       0.99      0.99      0.99      1115
{% endhighlight %}

## 6) Conclusion
* The naive bayes model achieved an accuracy of 98.5% on the test dataset. 
* It seems to be performing better than the training accuracy. we might want to consider getting more data or doing a stratified k-fold cross validation as the split on the training and testing data might not be even. As seen from the piechart above, messages labeled as spam only made up of 13.4% of the total observations. The split could have been not even, causing the learning not to be proportion and a low recall score(True positive rate) on spam messages.
* More spam messages seem to be missclassified.

Let's review which messages are missclassified:
### Missclassified as ham
```python
sms.iloc[y_test.index[((predictions == 'ham') & (y_test == 'spam'))],:]
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>869</th>
      <td>spam</td>
      <td>Hello. We need some posh birds and chaps to us...</td>
      <td>134</td>
    </tr>
    <tr>
      <th>5110</th>
      <td>spam</td>
      <td>You have 1 new message. Please call 08715205273</td>
      <td>47</td>
    </tr>
    <tr>
      <th>5427</th>
      <td>spam</td>
      <td>Santa Calling! Would your little ones like a c...</td>
      <td>106</td>
    </tr>
    <tr>
      <th>2430</th>
      <td>spam</td>
      <td>Guess who am I?This is the first time I create...</td>
      <td>152</td>
    </tr>
    <tr>
      <th>5381</th>
      <td>spam</td>
      <td>You have 1 new message. Call 0207-083-6089</td>
      <td>42</td>
    </tr>
    <tr>
      <th>1663</th>
      <td>spam</td>
      <td>Hi if ur lookin 4 saucy daytime fun wiv busty ...</td>
      <td>159</td>
    </tr>
    <tr>
      <th>1940</th>
      <td>spam</td>
      <td>More people are dogging in your area now. Call...</td>
      <td>159</td>
    </tr>
    <tr>
      <th>227</th>
      <td>spam</td>
      <td>Will u meet ur dream partner soon? Is ur caree...</td>
      <td>137</td>
    </tr>
    <tr>
      <th>4213</th>
      <td>spam</td>
      <td>Missed call alert. These numbers called but le...</td>
      <td>72</td>
    </tr>
    <tr>
      <th>191</th>
      <td>spam</td>
      <td>Are you unique enough? Find out from 30th Augu...</td>
      <td>72</td>
    </tr>
    <tr>
      <th>2804</th>
      <td>spam</td>
      <td>FreeMsg&gt;FAV XMAS TONES!Reply REAL</td>
      <td>33</td>
    </tr>
  </tbody>
</table>
### Missclassified as spam
```python
sms.iloc[y_test.index[((predictions == 'spam') & (y_test == 'ham'))],:]
```
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>message</th>
      <th>length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2570</th>
      <td>ham</td>
      <td>Ultimately tor motive tui achieve korli.</td>
      <td>40</td>
    </tr>
    <tr>
      <th>2318</th>
      <td>ham</td>
      <td>Waqt se pehle or naseeb se zyada kisi ko kuch ...</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2251</th>
      <td>ham</td>
      <td>I am getting threats from your sales executive...</td>
      <td>113</td>
    </tr>
    <tr>
      <th>2236</th>
      <td>ham</td>
      <td>Si.como no?!listened2the plaid album-quite gd&amp;...</td>
      <td>158</td>
    </tr>
    <tr>
      <th>1290</th>
      <td>ham</td>
      <td>Hey...Great deal...Farm tour 9am to 5pm $95/pa...</td>
      <td>70</td>
    </tr>
  </tbody>
</table>
At one glance, it is hard even for us to distinguish some of these messages from spam or ham. There are some subtleness in the context of the messages that induce this difficulty. We might need to consider having more data so that the machine can improve on it's learning and performance.

